{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\r\n",
        "import tqdm\r\n",
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch\r\n",
        "from datasets import load_dataset\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\r\n",
        "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification,Trainer, TrainingArguments,BertForSequenceClassification,BertTokenizerFast, GPT2TokenizerFast,GPT2Tokenizer,GPT2LMHeadModel"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "2023-06-18 10:06:57.605361: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1687082818302
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Link for dataset download.\r\n",
        "# https://github.com/commonsense/conceptnet5/wiki/Downloads\r\n",
        "\r\n",
        "# DATA IS TO HEAVY WE SHOULD LOAD IT OURSELVES"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687082818621
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !wget https://s3.amazonaws.com/conceptnet/downloads/2019/edges/conceptnet-assertions-5.7.0.csv.gz"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687082818913
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !gzip -d 'conceptnet-assertions-5.7.0.csv.gz'"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687082819067
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# s=0\r\n",
        "# with open('conceptnet-assertions-5.7.0.csv') as f:\r\n",
        "#     for line in f:\r\n",
        "#         s+=1\r\n",
        "#     if s%1000000==0:\r\n",
        "#         print(s)\r\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687082819185
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_english(cnt,nrows=1000000,types=['RelatedTo','IsA','ObstructedBy',\r\n",
        "    'HasProperty','HasPrerequisite','Causes','UsedFor','HasA','MadeOf',\r\n",
        "    'CreatedBy','AtLocation']):\r\n",
        "    data=pd.read_csv('conceptnet-assertions-5.7.0.csv',sep='\\t', \\\r\n",
        "    skiprows=cnt*nrows,nrows=nrows,names=[0,1,2,3,4])\r\n",
        "\r\n",
        "    data=data[(data[2].apply(lambda x: '/c/en/' in x)) & \\\r\n",
        "     (data[3].apply(lambda x: '/c/en/' in x))]\r\n",
        "\r\n",
        "    data['t']=False\r\n",
        "\r\n",
        "    for t in types:\r\n",
        "            data['t']=(data[1].apply(lambda x:t in x)) | data['t']\r\n",
        "\r\n",
        "    data=data[data['t']]\r\n",
        "    data=data.drop(['t'], axis=1)\r\n",
        "    if data.shape[0]>0:\r\n",
        "        print(data.shape[0])\r\n",
        "    data=data[[1,2,3]]\r\n",
        "    data[1]=data[1].apply(lambda x:x.replace('/r/',''))\r\n",
        "    data[2]=data[2].apply(lambda x:x.replace('/c/en/',''))\r\n",
        "    data[3]=data[3].apply(lambda x:x.replace('/c/en/',''))\r\n",
        "    data[2]=data[2].apply(lambda x:x.lower())\r\n",
        "    data[3]=data[3].apply(lambda x:x.lower())\r\n",
        "    return data\r\n",
        "def read_all():\r\n",
        "    dfs=[]\r\n",
        "    for cnt in tqdm.tqdm(range(35)):\r\n",
        "        dfs.append(read_english(cnt))\r\n",
        "    dfs=pd.concat(dfs,axis=0)\r\n",
        "    dfs=dfs.sort_values(by=[2])\r\n",
        "    dfs=dfs.reset_index(drop=True)\r\n",
        "    return dfs"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687082819297
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data=read_all()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 35/35 [09:50<00:00, 16.89s/it]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "49549\n32075\n5545\n31143\n231009\n241815\n950692\n511075\n17524\n22266\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083420367
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length=5\r\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2',max_length=max_length,add_prefix_space=True)\r\n",
        "vocab = tokenizer.get_vocab()\r\n",
        "for token in data[1].unique():\r\n",
        "  tokenizer.add_tokens([f\"<{token}>\"])\r\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\r\n",
        "num_added_tokens=data[1].unique().shape[0]+1\r\n",
        "v_max=len(tokenizer)\r\n",
        "print(v_max)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "50271\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083420927
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_in_db(word):\r\n",
        "    left=data[2].searchsorted(word,'left')\r\n",
        "    right=data[2].searchsorted(word,'right')\r\n",
        "    sel=data.iloc[left:right]\r\n",
        "    return sel"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083421052
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_in_db_2(word):\r\n",
        "    sel=find_in_db(word)\r\n",
        "    sel=sel[sel[3].apply(lambda x: ('Ġ'+x in vocab))]\r\n",
        "    return sel"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083421173
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df=[]\r\n",
        "for word in tqdm.tqdm(vocab.keys()):\r\n",
        "    df.append(find_in_db_2(word))\r\n",
        "df=pd.concat(df,axis=0)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 50257/50257 [00:41<00:00, 1199.49it/s]\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083463218
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "                               1          2            3\n934432                 RelatedTo       info  information\n934433   EtymologicallyRelatedTo       info  information\n115595                 RelatedTo         at         rate\n115596                 RelatedTo         at    regarding\n115597                 RelatedTo         at        shift\n...                          ...        ...          ...\n1561599                RelatedTo  represent         gift\n1561600                RelatedTo  represent       behalf\n1561601                RelatedTo  represent        again\n1561603                RelatedTo  represent          act\n1561604                RelatedTo  represent      present\n\n[56077 rows x 3 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>934432</th>\n      <td>RelatedTo</td>\n      <td>info</td>\n      <td>information</td>\n    </tr>\n    <tr>\n      <th>934433</th>\n      <td>EtymologicallyRelatedTo</td>\n      <td>info</td>\n      <td>information</td>\n    </tr>\n    <tr>\n      <th>115595</th>\n      <td>RelatedTo</td>\n      <td>at</td>\n      <td>rate</td>\n    </tr>\n    <tr>\n      <th>115596</th>\n      <td>RelatedTo</td>\n      <td>at</td>\n      <td>regarding</td>\n    </tr>\n    <tr>\n      <th>115597</th>\n      <td>RelatedTo</td>\n      <td>at</td>\n      <td>shift</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1561599</th>\n      <td>RelatedTo</td>\n      <td>represent</td>\n      <td>gift</td>\n    </tr>\n    <tr>\n      <th>1561600</th>\n      <td>RelatedTo</td>\n      <td>represent</td>\n      <td>behalf</td>\n    </tr>\n    <tr>\n      <th>1561601</th>\n      <td>RelatedTo</td>\n      <td>represent</td>\n      <td>again</td>\n    </tr>\n    <tr>\n      <th>1561603</th>\n      <td>RelatedTo</td>\n      <td>represent</td>\n      <td>act</td>\n    </tr>\n    <tr>\n      <th>1561604</th>\n      <td>RelatedTo</td>\n      <td>represent</td>\n      <td>present</td>\n    </tr>\n  </tbody>\n</table>\n<p>56077 rows × 3 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083463457
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(frac=1)\r\n",
        "train=df.iloc[:int(df.shape[0]*0.8)]\r\n",
        "test=df.iloc[int(df.shape[0]*0.8):]"
      ],
      "outputs": [],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083463567
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class KnowledgeDataset(Dataset):\r\n",
        "    def __init__(self, df):\r\n",
        "        self.df = df\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.df)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        row=self.df.iloc[idx]\r\n",
        "        input_=row[2]+f\" <{row[1]}> \"+row[3]\r\n",
        "        input_ids=torch.tensor(tokenizer.encode(input_,padding='max_length',max_length=max_length,truncation=True))\r\n",
        "        # mask_s=torch.where(input_ids>=v_max-num_added_tokens,1,0).nonzero()[0].item()\r\n",
        "        mask1=torch.where(input_ids!=v_max-1,1,0)\r\n",
        "        # l=int(mask1.sum().item())\r\n",
        "        # mask2=torch.tensor([float(i>=mask_s) for i in range(max_length)])\r\n",
        "        # attention_mask=mask1*mask2\r\n",
        "        # attention_mask[l-1]=0\r\n",
        "        return {'input_ids':input_ids,\r\n",
        "         'attention_mask': mask1}\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083463927
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_p=KnowledgeDataset(train)\r\n",
        "test_p=KnowledgeDataset(test)"
      ],
      "outputs": [],
      "execution_count": 15,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083464118
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_p[1]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "{'input_ids': tensor([ 4656,   220, 50259,  6427, 50270]),\n 'attention_mask': tensor([1, 1, 1, 1, 0])}"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083464249
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mini_batch(samples):\r\n",
        "    input_ids = [s['input_ids'] for s in samples]\r\n",
        "    attention_mask = [(s['attention_mask']) for s in samples]\r\n",
        "    l=max_length\r\n",
        "    input_ids=torch.stack(input_ids)[:,:l]\r\n",
        "    attention_mask=torch.stack(attention_mask)[:,:l]\r\n",
        "\r\n",
        "    return {'input_ids':input_ids, 'attention_mask':attention_mask}"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083464367
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size_gpt2 = 128\r\n",
        "train_loader = DataLoader(train_p, batch_size=batch_size_gpt2, shuffle=True,collate_fn=create_mini_batch)\r\n",
        "test_loader = DataLoader(test_p, batch_size=batch_size_gpt2,collate_fn=create_mini_batch)"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083464513
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\r\n",
        "model.resize_token_embeddings(len(tokenizer))\r\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
        "# device='cpu'\r\n",
        "model.to(device)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50271, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50271, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083467964
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_response_full(input_,num=8 ):\r\n",
        "    input_ids=torch.tensor(tokenizer.encode(input_,padding='max_length',max_length=max_length,truncation=True)).to(device)\r\n",
        "    # mask_s=torch.where(input_ids>=v_max-num_added_tokens,1,0).nonzero()[0].item()\r\n",
        "    mask1=torch.where(input_ids!=v_max-1,1,0).to(device)\r\n",
        "    # l=int(mask1.sum().item())\r\n",
        "    # mask2=torch.tensor([int(i>=mask_s) for i in range(max_length)]).to(device)\r\n",
        "    # attention_mask=mask1*mask2\r\n",
        "    # attention_mask[l-1]=0\r\n",
        "    # attention_mask=attention_mask.to(device)\r\n",
        "    outputs = model(input_ids=input_ids, attention_mask=mask1)\r\n",
        "    outputs= outputs.logits[mask1.sum().item()-1]\r\n",
        "    sorted, indices = torch.sort(outputs,descending=True)\r\n",
        "    choice=np.random.choice(np.arange(0,num), p=F.softmax(sorted[:num].type(torch.FloatTensor), dim=-1).detach().numpy())\r\n",
        "    print(tokenizer.convert_ids_to_tokens(torch.topk(outputs, num)[1]))\r\n",
        "    # print(choice)\r\n",
        "\r\n",
        "    outputs=tokenizer.convert_ids_to_tokens([indices[choice]])[0]\r\n",
        "    new_token=outputs.replace('Ġ','')\r\n",
        "    input_=input_+' '+new_token\r\n",
        "    return input_\r\n",
        "def generate_response(input_=' hello <IsA>',l=1,num=1):\r\n",
        "    for _ in range(l):\r\n",
        "        # input_=add_response_old(input_)\r\n",
        "        # input_=add_response(input_)\r\n",
        "        input_=add_response_full(input_,num)\r\n",
        "\r\n",
        "    return input_"
      ],
      "outputs": [],
      "execution_count": 67,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687101310801
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval():\r\n",
        "    with torch.no_grad():\r\n",
        "        cnt=0\r\n",
        "        loss=0\r\n",
        "        for batch in tqdm.tqdm(test_loader):\r\n",
        "            # input_ids,attention_mask,labels=batch\r\n",
        "            input_ids=batch['input_ids']\r\n",
        "            attention_mask=batch['attention_mask']\r\n",
        "            input_ids = input_ids.to(device)\r\n",
        "            attention_mask = attention_mask.to(device)\r\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask,labels=input_ids)\r\n",
        "            loss+=outputs.loss.item()\r\n",
        "            cnt+=1\r\n",
        "        loss=loss/cnt\r\n",
        "        print(generate_response(\" man <HasA>\",1,1))\r\n",
        "        return loss"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083468187
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eval()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 88/88 [00:14<00:00,  6.12it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " man <HasA> <CreatedBy>\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "43.61714714223688"
          },
          "metadata": {}
        }
      ],
      "execution_count": 22,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083483117
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs=200):\r\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\r\n",
        "    best_valid_loss = float('inf')\r\n",
        "    loss_fn=nn.CrossEntropyLoss()\r\n",
        "    for epoch in range(num_epochs):\r\n",
        "        model.train()\r\n",
        "        train_loss = 0.0\r\n",
        "        corrects=0.0\r\n",
        "        total=0.0\r\n",
        "        for batch in tqdm.tqdm(train_loader):\r\n",
        "            input_ids=batch['input_ids']\r\n",
        "            attention_mask=batch['attention_mask']\r\n",
        "            input_ids = input_ids.to(device)\r\n",
        "            attention_mask = attention_mask.to(device)\r\n",
        "\r\n",
        "            optimizer.zero_grad()\r\n",
        "\r\n",
        "            outputs = model(input_ids=input_ids,attention_mask=attention_mask,labels=input_ids)\r\n",
        "            loss = outputs.loss\r\n",
        "            train_loss += loss.item()\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "        avg_train_loss = train_loss / len(train_loader)\r\n",
        "\r\n",
        "        model.eval()\r\n",
        "        avg_valid_loss = eval()\r\n",
        "        if avg_valid_loss < best_valid_loss:\r\n",
        "            best_valid_loss = avg_valid_loss\r\n",
        "            torch.save(model.state_dict(), \"connet.pt\")\r\n",
        "\r\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}: \"\r\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687083483217
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.11it/s]\n100%|██████████| 351/351 [02:32<00:00,  2.30it/s]\n100%|██████████| 88/88 [00:14<00:00,  5.99it/s]\n100%|██████████| 351/351 [02:32<00:00,  2.30it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.05it/s]\n100%|██████████| 351/351 [02:32<00:00,  2.30it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.07it/s]\n100%|██████████| 351/351 [02:32<00:00,  2.31it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n100%|██████████| 351/351 [02:32<00:00,  2.30it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.01it/s]\n100%|██████████| 351/351 [02:32<00:00,  2.31it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.03it/s]\n100%|██████████| 351/351 [02:32<00:00,  2.31it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.03it/s]\n100%|██████████| 351/351 [02:31<00:00,  2.32it/s]\n100%|██████████| 88/88 [00:14<00:00,  5.94it/s]\n100%|██████████| 351/351 [02:31<00:00,  2.31it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.18it/s]\n100%|██████████| 351/351 [02:31<00:00,  2.32it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.05it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:31<00:00,  2.32it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.24it/s]\n100%|██████████| 351/351 [02:31<00:00,  2.32it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n100%|██████████| 351/351 [02:31<00:00,  2.32it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.13it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.13it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.18it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.22it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.21it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.15it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.10it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.22it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.18it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.15it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.21it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.28it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.21it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.04it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.23it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n 58%|█████▊    | 205/351 [01:27<01:01,  2.36it/s]IOPub message rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_msg_rate_limit`.\n\nCurrent values:\nServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.18it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.15it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.21it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.18it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.14it/s]\n 18%|█▊        | 63/351 [00:26<02:02,  2.35it/s]IOPub message rate exceeded.\nThe Jupyter server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--ServerApp.iopub_msg_rate_limit`.\n\nCurrent values:\nServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nServerApp.rate_limit_window=3.0 (secs)\n\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.22it/s]\n100%|██████████| 351/351 [02:29<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.15it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.13it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n100%|██████████| 351/351 [02:29<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.21it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.14it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.15it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.21it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.07it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.13it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.12it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:29<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.06it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.14it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.24it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.21it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.16it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.20it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.17it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.33it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.18it/s]\n100%|██████████| 351/351 [02:29<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.23it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.19it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n100%|██████████| 88/88 [00:14<00:00,  6.12it/s]\n100%|██████████| 351/351 [02:30<00:00,  2.34it/s]\n 86%|████████▋ | 76/88 [00:12<00:01,  6.09it/s]\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " man <HasA> person\nEpoch 1/200: Train Loss: 5.2404, Valid Loss: 2.7086\n man <HasA> person\nEpoch 2/200: Train Loss: 2.8330, Valid Loss: 2.2695\n man <HasA> person\nEpoch 3/200: Train Loss: 2.5882, Valid Loss: 2.2030\n man <HasA> man\nEpoch 4/200: Train Loss: 2.4610, Valid Loss: 2.1917\n man <HasA> man\nEpoch 5/200: Train Loss: 2.3672, Valid Loss: 2.1855\n man <HasA> man\nEpoch 6/200: Train Loss: 2.2805, Valid Loss: 2.1723\n man <HasA> man\nEpoch 7/200: Train Loss: 2.2042, Valid Loss: 2.1552\n man <HasA> man\nEpoch 8/200: Train Loss: 2.1290, Valid Loss: 2.1278\n man <HasA> man\nEpoch 9/200: Train Loss: 2.0798, Valid Loss: 2.1109\n man <HasA> man\nEpoch 10/200: Train Loss: 2.0487, Valid Loss: 2.1062\n man <HasA> man\nEpoch 11/200: Train Loss: 2.0244, Valid Loss: 2.0958\n man <HasA> man\nEpoch 12/200: Train Loss: 2.0076, Valid Loss: 2.0914\n man <HasA> man\nEpoch 13/200: Train Loss: 1.9872, Valid Loss: 2.0858\n man <HasA> man\nEpoch 14/200: Train Loss: 1.9688, Valid Loss: 2.0722\n man <HasA> man\nEpoch 15/200: Train Loss: 1.9507, Valid Loss: 2.0688\n man <HasA> man\nEpoch 16/200: Train Loss: 1.9343, Valid Loss: 2.0638\n man <HasA> man\nEpoch 17/200: Train Loss: 1.9190, Valid Loss: 2.0491\n man <HasA> man\nEpoch 18/200: Train Loss: 1.9033, Valid Loss: 2.0525\n man <HasA> man\nEpoch 19/200: Train Loss: 1.8869, Valid Loss: 2.0478\n man <HasA> man\nEpoch 20/200: Train Loss: 1.8728, Valid Loss: 2.0467\n man <HasA> man\nEpoch 21/200: Train Loss: 1.8576, Valid Loss: 2.0420\n man <HasA> man\nEpoch 22/200: Train Loss: 1.8448, Valid Loss: 2.0407\n man <HasA> man\nEpoch 23/200: Train Loss: 1.8319, Valid Loss: 2.0371\n man <HasA> man\nEpoch 24/200: Train Loss: 1.8188, Valid Loss: 2.0390\n man <HasA> man\nEpoch 25/200: Train Loss: 1.8035, Valid Loss: 2.0373\n man <HasA> man\nEpoch 26/200: Train Loss: 1.7910, Valid Loss: 2.0343\n man <HasA> penis\nEpoch 27/200: Train Loss: 1.7767, Valid Loss: 2.0416\n man <HasA> penis\nEpoch 28/200: Train Loss: 1.7640, Valid Loss: 2.0400\n man <HasA> penis\nEpoch 29/200: Train Loss: 1.7510, Valid Loss: 2.0387\n man <HasA> penis\nEpoch 30/200: Train Loss: 1.7404, Valid Loss: 2.0403\n man <HasA> penis\nEpoch 31/200: Train Loss: 1.7259, Valid Loss: 2.0400\n man <HasA> penis\nEpoch 32/200: Train Loss: 1.7151, Valid Loss: 2.0475\n man <HasA> legs\nEpoch 33/200: Train Loss: 1.7050, Valid Loss: 2.0483\n man <HasA> legs\nEpoch 34/200: Train Loss: 1.6910, Valid Loss: 2.0530\n man <HasA> legs\nEpoch 35/200: Train Loss: 1.6790, Valid Loss: 2.0648\n man <HasA> legs\nEpoch 36/200: Train Loss: 1.6700, Valid Loss: 2.0627\n man <HasA> legs\nEpoch 37/200: Train Loss: 1.6581, Valid Loss: 2.0712\n man <HasA> legs\nEpoch 38/200: Train Loss: 1.6494, Valid Loss: 2.0718\n man <HasA> legs\nEpoch 39/200: Train Loss: 1.6385, Valid Loss: 2.0800\n man <HasA> legs\nEpoch 40/200: Train Loss: 1.6287, Valid Loss: 2.0822\n man <HasA> hair\nEpoch 41/200: Train Loss: 1.6163, Valid Loss: 2.0855\n man <HasA> legs\nEpoch 42/200: Train Loss: 1.6053, Valid Loss: 2.0930\n man <HasA> legs\nEpoch 43/200: Train Loss: 1.5954, Valid Loss: 2.1022\n man <HasA> legs\nEpoch 44/200: Train Loss: 1.5871, Valid Loss: 2.1061\n man <HasA> legs\nEpoch 45/200: Train Loss: 1.5754, Valid Loss: 2.1166\n man <HasA> legs\nEpoch 46/200: Train Loss: 1.5650, Valid Loss: 2.1237\n man <HasA> legs\nEpoch 47/200: Train Loss: 1.5574, Valid Loss: 2.1250\n man <HasA> legs\nEpoch 48/200: Train Loss: 1.5483, Valid Loss: 2.1375\n man <HasA> legs\nEpoch 49/200: Train Loss: 1.5376, Valid Loss: 2.1523\n man <HasA> legs\nEpoch 50/200: Train Loss: 1.5288, Valid Loss: 2.1500\n man <HasA> muscles\nEpoch 51/200: Train Loss: 1.5200, Valid Loss: 2.1689\n man <HasA> penis\nEpoch 52/200: Train Loss: 1.5113, Valid Loss: 2.1769\n man <HasA> teeth\nEpoch 65/200: Train Loss: 1.4054, Valid Loss: 2.2850\n man <HasA> teeth\nEpoch 66/200: Train Loss: 1.3973, Valid Loss: 2.2713\n man <HasA> penis\nEpoch 67/200: Train Loss: 1.3911, Valid Loss: 2.2867\n man <HasA> penis\nEpoch 68/200: Train Loss: 1.3843, Valid Loss: 2.2984\n man <HasA> penis\nEpoch 69/200: Train Loss: 1.3768, Valid Loss: 2.3011\n man <HasA> penis\nEpoch 70/200: Train Loss: 1.3704, Valid Loss: 2.3073\n man <HasA> penis\nEpoch 71/200: Train Loss: 1.3652, Valid Loss: 2.3176\n man <HasA> penis\nEpoch 77/200: Train Loss: 1.3267, Valid Loss: 2.3570\n man <HasA> penis\nEpoch 78/200: Train Loss: 1.3207, Valid Loss: 2.3692\n man <HasA> penis\nEpoch 79/200: Train Loss: 1.3157, Valid Loss: 2.3671\n man <HasA> penis\nEpoch 80/200: Train Loss: 1.3133, Valid Loss: 2.3754\n man <HasA> penis\nEpoch 81/200: Train Loss: 1.3063, Valid Loss: 2.3844\n man <HasA> penis\nEpoch 82/200: Train Loss: 1.3011, Valid Loss: 2.3843\n man <HasA> teeth\nEpoch 83/200: Train Loss: 1.2937, Valid Loss: 2.3883\n man <HasA> teeth\nEpoch 84/200: Train Loss: 1.2904, Valid Loss: 2.3902\n man <HasA> penis\nEpoch 85/200: Train Loss: 1.2845, Valid Loss: 2.3995\n man <HasA> penis\nEpoch 86/200: Train Loss: 1.2802, Valid Loss: 2.4070\n man <HasA> penis\nEpoch 87/200: Train Loss: 1.2752, Valid Loss: 2.4081\n man <HasA> penis\nEpoch 88/200: Train Loss: 1.2714, Valid Loss: 2.4120\n man <HasA> penis\nEpoch 89/200: Train Loss: 1.2677, Valid Loss: 2.4251\n man <HasA> teeth\nEpoch 90/200: Train Loss: 1.2630, Valid Loss: 2.4273\n man <HasA> penis\nEpoch 91/200: Train Loss: 1.2581, Valid Loss: 2.4365\n man <HasA> penis\nEpoch 92/200: Train Loss: 1.2540, Valid Loss: 2.4373\n man <HasA> penis\nEpoch 93/200: Train Loss: 1.2502, Valid Loss: 2.4371\n man <HasA> penis\nEpoch 94/200: Train Loss: 1.2440, Valid Loss: 2.4457\n man <HasA> penis\nEpoch 95/200: Train Loss: 1.2437, Valid Loss: 2.4287\n man <HasA> penis\nEpoch 96/200: Train Loss: 1.2378, Valid Loss: 2.4598\n man <HasA> penis\nEpoch 97/200: Train Loss: 1.2367, Valid Loss: 2.4697\n man <HasA> penis\nEpoch 98/200: Train Loss: 1.2311, Valid Loss: 2.4687\n man <HasA> penis\nEpoch 99/200: Train Loss: 1.2258, Valid Loss: 2.4710\n man <HasA> penis\nEpoch 100/200: Train Loss: 1.2241, Valid Loss: 2.4849\n man <HasA> penis\nEpoch 101/200: Train Loss: 1.2232, Valid Loss: 2.4713\n man <HasA> penis\nEpoch 102/200: Train Loss: 1.2182, Valid Loss: 2.4843\n man <HasA> penis\nEpoch 103/200: Train Loss: 1.2138, Valid Loss: 2.4775\n man <HasA> penis\nEpoch 104/200: Train Loss: 1.2123, Valid Loss: 2.4879\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(num_epochs)\u001b[0m\n\u001b[1;32m     23\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m train_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 26\u001b[0m avg_valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m avg_valid_loss \u001b[38;5;241m<\u001b[39m best_valid_loss:\n\u001b[1;32m     28\u001b[0m     best_valid_loss \u001b[38;5;241m=\u001b[39m avg_valid_loss\n",
            "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36meval\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,labels\u001b[38;5;241m=\u001b[39minput_ids)\n\u001b[0;32m---> 12\u001b[0m     loss\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     cnt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     14\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m/\u001b[39mcnt\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 24,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687100899529
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"connet_2.pt\")"
      ],
      "outputs": [],
      "execution_count": 50,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687101036555
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "address=\"connet_t.pt\"\r\n",
        "model.load_state_dict(torch.load(address))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 119,
          "data": {
            "text/plain": "<All keys matched successfully>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 119,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687082585063
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 120,
          "data": {
            "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50271, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50271, bias=False)\n)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 120,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687082585208
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(\" school <CreatedBy>\",1,5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['Ġbuilding', 'Ġchildren', 'Ġclasses', 'Ġplace', 'Ġshelter']\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 80,
          "data": {
            "text/plain": "' school <CreatedBy> children'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 80,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687101612703
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(\" male <UsedFor>\",1,5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "['Ġsex', 'Ġlove', 'Ġreproduction', 'Ġfun', 'Ġpleasure']\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 74,
          "data": {
            "text/plain": "' male <UsedFor> reproduction'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 74,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1687101466928
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}